{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4012 Fraud Analytics Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training dataset\n",
    "train = pd.read_csv(\"fraudTrain.csv\", index_col=0)\n",
    "\n",
    "# Import the testing dataset\n",
    "test = pd.read_csv(\"fraudTest.csv\", index_col=0)\n",
    "\n",
    "validation = train[len(train) - round(len(train)*0.1):]\n",
    "train = train[:len(train) - round(len(train)*0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = pd.concat([train[\"cc_num\"], train[\"merchant\"], test[\"cc_num\"], test[\"merchant\"], validation[\"cc_num\"], validation[\"merchant\"]]).unique()\n",
    "mapping = {node: i for i, node in enumerate(all_nodes)}\n",
    "num_nodes = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"transaction_dt\"] = pd.to_datetime(train[\"trans_date_trans_time\"])\n",
    "test[\"transaction_dt\"] = pd.to_datetime(test[\"trans_date_trans_time\"])\n",
    "validation[\"transaction_dt\"] = pd.to_datetime(validation[\"trans_date_trans_time\"])\n",
    "\n",
    "train[\"transaction_hour\"] = train[\"transaction_dt\"].dt.hour\n",
    "validation[\"transaction_hour\"] = validation[\"transaction_dt\"].dt.hour\n",
    "test[\"transaction_hour\"] = test[\"transaction_dt\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leejw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leejw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Fit encoders on the training data\n",
    "category_encoder = OneHotEncoder(sparse=False).fit(train[['category']])\n",
    "state_encoder = OneHotEncoder(sparse=False).fit(train[['state']])\n",
    "\n",
    "def process_data(df, mapping, category_encoder, state_encoder):\n",
    "    df[\"from\"] = df[\"cc_num\"].map(mapping)\n",
    "    df[\"to\"] = df[\"merchant\"].map(mapping)\n",
    "    edge_index = torch.tensor([df[\"from\"].values, df[\"to\"].values], dtype=torch.long)\n",
    "    \n",
    "    # Transform 'category' and 'state' using the fitted encoders\n",
    "    category_features = category_encoder.transform(df[['category']])\n",
    "    state_features = state_encoder.transform(df[['state']])\n",
    "    \n",
    "    # Normalize 'transaction_hour'\n",
    "    transaction_hours = df['transaction_hour'].values / 24.0  # Normalize hours to [0, 1]\n",
    "    transaction_hours = transaction_hours.reshape(-1, 1)\n",
    "\n",
    "    edge_features = torch.tensor(df[['amt']].values, dtype=torch.float)\n",
    "    \n",
    "    # Concatenate all features to create node features\n",
    "    node_features = np.concatenate((category_features, state_features, transaction_hours), axis=1)\n",
    "    node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Create the data object\n",
    "    data = Data(x=node_features_tensor, edge_index=edge_index, edge_attr=edge_features)\n",
    "    return data\n",
    "\n",
    "# Create a node mapping for all nodes\n",
    "all_nodes = pd.concat([train[\"cc_num\"], train[\"merchant\"], test[\"cc_num\"], test[\"merchant\"], validation[\"cc_num\"], validation[\"merchant\"]]).unique()\n",
    "mapping = {node: i for i, node in enumerate(all_nodes)}\n",
    "\n",
    "# Identify the indices of the fraud and non-fraud instances\n",
    "fraud_indices = train[train['is_fraud'] == 1].index\n",
    "non_fraud_indices = train[train['is_fraud'] == 0].index\n",
    "\n",
    "# Randomly sample the non-fraud instances to match the number of fraud instances\n",
    "undersampled_non_fraud_indices = np.random.choice(non_fraud_indices, size=len(fraud_indices), replace=False)\n",
    "undersampled_indices = np.concatenate([fraud_indices, undersampled_non_fraud_indices])\n",
    "\n",
    "# Create the undersampled training set\n",
    "undersampled_train = train.loc[undersampled_indices]\n",
    "\n",
    "data_train = process_data(undersampled_train, mapping, category_encoder, state_encoder)\n",
    "data_val = process_data(validation, mapping, category_encoder, state_encoder)\n",
    "data_test = process_data(test, mapping, category_encoder, state_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6985903382301331, Val Loss: 0.6697142124176025, Val AUC: 0.5897166970770233\n",
      "Epoch 2, Loss: 0.6804229021072388, Val Loss: 0.697489857673645, Val AUC: 0.5954144968439303\n",
      "Epoch 3, Loss: 0.6788742542266846, Val Loss: 0.7110375761985779, Val AUC: 0.619621479125519\n",
      "Epoch 4, Loss: 0.6650028228759766, Val Loss: 0.7126448154449463, Val AUC: 0.6317489646945312\n",
      "Epoch 5, Loss: 0.6597253084182739, Val Loss: 0.7038187384605408, Val AUC: 0.6388463150528877\n",
      "Epoch 6, Loss: 0.6493579745292664, Val Loss: 0.6892430782318115, Val AUC: 0.6475338466971725\n",
      "Epoch 7, Loss: 0.641473114490509, Val Loss: 0.6709657311439514, Val AUC: 0.6548558789409076\n",
      "Epoch 8, Loss: 0.6381662487983704, Val Loss: 0.6509443521499634, Val AUC: 0.6583313580532913\n",
      "Epoch 9, Loss: 0.6301102638244629, Val Loss: 0.6307270526885986, Val AUC: 0.6665229550925804\n",
      "Epoch 10, Loss: 0.6218758225440979, Val Loss: 0.6121864914894104, Val AUC: 0.6731768936417676\n",
      "Epoch 11, Loss: 0.6195500493049622, Val Loss: 0.5972059369087219, Val AUC: 0.6742559525586495\n",
      "Epoch 12, Loss: 0.6133149862289429, Val Loss: 0.5862327814102173, Val AUC: 0.6787273310493722\n",
      "Epoch 13, Loss: 0.6087074279785156, Val Loss: 0.5791649222373962, Val AUC: 0.6795950860008072\n",
      "Epoch 14, Loss: 0.603367805480957, Val Loss: 0.5763564705848694, Val AUC: 0.6788522662681477\n",
      "Epoch 15, Loss: 0.6008684039115906, Val Loss: 0.5780120491981506, Val AUC: 0.6811656948487881\n",
      "Epoch 16, Loss: 0.6008006930351257, Val Loss: 0.5810867547988892, Val AUC: 0.6811495928335844\n",
      "Epoch 17, Loss: 0.5954303741455078, Val Loss: 0.5842369198799133, Val AUC: 0.6816917624254568\n",
      "Epoch 18, Loss: 0.58885258436203, Val Loss: 0.586344301700592, Val AUC: 0.6814893770732977\n",
      "Epoch 19, Loss: 0.5904008150100708, Val Loss: 0.5848367810249329, Val AUC: 0.6823586993124233\n",
      "Epoch 20, Loss: 0.5879061222076416, Val Loss: 0.5805020928382874, Val AUC: 0.6841089864992345\n",
      "Epoch 21, Loss: 0.5894278287887573, Val Loss: 0.5746656656265259, Val AUC: 0.685269954856144\n",
      "Epoch 22, Loss: 0.5860141515731812, Val Loss: 0.5673914551734924, Val AUC: 0.6868112516926708\n",
      "Epoch 23, Loss: 0.5759689211845398, Val Loss: 0.5593956112861633, Val AUC: 0.687208838995073\n",
      "Epoch 24, Loss: 0.5818969011306763, Val Loss: 0.5532392859458923, Val AUC: 0.6886486539536251\n",
      "Epoch 25, Loss: 0.5801037549972534, Val Loss: 0.5491554737091064, Val AUC: 0.6873042942787133\n",
      "Epoch 26, Loss: 0.5767079591751099, Val Loss: 0.5465822219848633, Val AUC: 0.6876425112307358\n",
      "Epoch 27, Loss: 0.5743517279624939, Val Loss: 0.545935332775116, Val AUC: 0.6905469938526498\n",
      "Epoch 28, Loss: 0.5701611638069153, Val Loss: 0.547186553478241, Val AUC: 0.6902054744015648\n",
      "Epoch 29, Loss: 0.5719711780548096, Val Loss: 0.5494042038917542, Val AUC: 0.6908175002447955\n",
      "Epoch 30, Loss: 0.5715561509132385, Val Loss: 0.5522230863571167, Val AUC: 0.6915343104650635\n",
      "Epoch 31, Loss: 0.5695164203643799, Val Loss: 0.5558015704154968, Val AUC: 0.6924606894394066\n",
      "Epoch 32, Loss: 0.5674312710762024, Val Loss: 0.5579846501350403, Val AUC: 0.6929491047951242\n",
      "Epoch 33, Loss: 0.5663024187088013, Val Loss: 0.5586415529251099, Val AUC: 0.694136082664425\n",
      "Epoch 34, Loss: 0.565070629119873, Val Loss: 0.55843186378479, Val AUC: 0.6952149736576255\n",
      "Epoch 35, Loss: 0.5644934177398682, Val Loss: 0.5567666292190552, Val AUC: 0.6957788613789265\n",
      "Epoch 36, Loss: 0.5618165731430054, Val Loss: 0.5536519289016724, Val AUC: 0.6959208874968058\n",
      "Epoch 37, Loss: 0.5591210126876831, Val Loss: 0.5501176118850708, Val AUC: 0.698012694134702\n",
      "Epoch 38, Loss: 0.5571812987327576, Val Loss: 0.5466365814208984, Val AUC: 0.7001516500106277\n",
      "Epoch 39, Loss: 0.5543279051780701, Val Loss: 0.5435211062431335, Val AUC: 0.7001999560562384\n",
      "Epoch 40, Loss: 0.5546146035194397, Val Loss: 0.5406803488731384, Val AUC: 0.7009805959335303\n",
      "Epoch 41, Loss: 0.5532858371734619, Val Loss: 0.5382974743843079, Val AUC: 0.7010322044782037\n",
      "Epoch 42, Loss: 0.5524337291717529, Val Loss: 0.5365936756134033, Val AUC: 0.7015794117805104\n",
      "Epoch 43, Loss: 0.5525322556495667, Val Loss: 0.5344113707542419, Val AUC: 0.7009790286458395\n",
      "Epoch 44, Loss: 0.5499347448348999, Val Loss: 0.5322996377944946, Val AUC: 0.7015999731023579\n",
      "Epoch 45, Loss: 0.5468213558197021, Val Loss: 0.5298125743865967, Val AUC: 0.7018960785267854\n",
      "Epoch 46, Loss: 0.5466338396072388, Val Loss: 0.5273656249046326, Val AUC: 0.701993679501907\n",
      "Epoch 47, Loss: 0.5442197322845459, Val Loss: 0.525724470615387, Val AUC: 0.702754336461142\n",
      "Epoch 48, Loss: 0.5465852618217468, Val Loss: 0.5241983532905579, Val AUC: 0.702952840910448\n",
      "Epoch 49, Loss: 0.5380498170852661, Val Loss: 0.5228853821754456, Val AUC: 0.7015231013727649\n",
      "Epoch 50, Loss: 0.5387820601463318, Val Loss: 0.5223625898361206, Val AUC: 0.7004996998270909\n",
      "Epoch 51, Loss: 0.5358333587646484, Val Loss: 0.5216411352157593, Val AUC: 0.7008489810838825\n",
      "Epoch 52, Loss: 0.5366154313087463, Val Loss: 0.5202028751373291, Val AUC: 0.7022643538177337\n",
      "Epoch 53, Loss: 0.5354520678520203, Val Loss: 0.5173495411872864, Val AUC: 0.7032079916148616\n",
      "Epoch 54, Loss: 0.5325294733047485, Val Loss: 0.5140389204025269, Val AUC: 0.7027977727199994\n",
      "Epoch 55, Loss: 0.5304121375083923, Val Loss: 0.5102666616439819, Val AUC: 0.7034891518316668\n",
      "Epoch 56, Loss: 0.5309213995933533, Val Loss: 0.5061354637145996, Val AUC: 0.7052266395023369\n",
      "Epoch 57, Loss: 0.5273466110229492, Val Loss: 0.5023272633552551, Val AUC: 0.7065073374439061\n",
      "Epoch 58, Loss: 0.5257617235183716, Val Loss: 0.4997575581073761, Val AUC: 0.7078916629549313\n",
      "Epoch 59, Loss: 0.5258170366287231, Val Loss: 0.4980301856994629, Val AUC: 0.7081755472670086\n",
      "Epoch 60, Loss: 0.5197303891181946, Val Loss: 0.4977181553840637, Val AUC: 0.7080325322652293\n",
      "Epoch 61, Loss: 0.5255608558654785, Val Loss: 0.4973626732826233, Val AUC: 0.7075707048256937\n",
      "Epoch 62, Loss: 0.5235128402709961, Val Loss: 0.49588945508003235, Val AUC: 0.710161412720286\n",
      "Epoch 63, Loss: 0.5155062079429626, Val Loss: 0.4946310520172119, Val AUC: 0.7114952864942193\n",
      "Epoch 64, Loss: 0.5128577947616577, Val Loss: 0.49352794885635376, Val AUC: 0.7136014599359471\n",
      "Epoch 65, Loss: 0.5159041285514832, Val Loss: 0.49077677726745605, Val AUC: 0.7146298991920558\n",
      "Epoch 66, Loss: 0.5116652846336365, Val Loss: 0.4874466061592102, Val AUC: 0.7172400116009141\n",
      "Epoch 67, Loss: 0.5121205449104309, Val Loss: 0.4846227467060089, Val AUC: 0.7177290053604224\n",
      "Epoch 68, Loss: 0.5115177631378174, Val Loss: 0.48196902871131897, Val AUC: 0.7206218452737767\n",
      "Epoch 69, Loss: 0.5083978176116943, Val Loss: 0.4812767207622528, Val AUC: 0.7221769305103446\n",
      "Epoch 70, Loss: 0.5059035420417786, Val Loss: 0.4801259934902191, Val AUC: 0.7243446199939338\n",
      "Epoch 71, Loss: 0.507869303226471, Val Loss: 0.4791092574596405, Val AUC: 0.7249838121571371\n",
      "Epoch 72, Loss: 0.505141019821167, Val Loss: 0.47879308462142944, Val AUC: 0.7286250879173761\n",
      "Epoch 73, Loss: 0.5053756237030029, Val Loss: 0.4771204888820648, Val AUC: 0.7299950466245698\n",
      "Epoch 74, Loss: 0.5004380345344543, Val Loss: 0.4754325747489929, Val AUC: 0.7321516718033898\n",
      "Epoch 75, Loss: 0.4985761344432831, Val Loss: 0.47232717275619507, Val AUC: 0.7318811654112444\n",
      "Epoch 76, Loss: 0.4972243309020996, Val Loss: 0.47051987051963806, Val AUC: 0.7339246660035298\n",
      "Epoch 77, Loss: 0.500123143196106, Val Loss: 0.47018176317214966, Val AUC: 0.7346342928218821\n",
      "Epoch 78, Loss: 0.49479275941848755, Val Loss: 0.47060349583625793, Val AUC: 0.7378325672530611\n",
      "Epoch 79, Loss: 0.4944058954715729, Val Loss: 0.470528781414032, Val AUC: 0.7407791614025702\n",
      "Epoch 80, Loss: 0.49155041575431824, Val Loss: 0.4695017337799072, Val AUC: 0.7414072892610044\n",
      "Epoch 81, Loss: 0.49295949935913086, Val Loss: 0.4675258994102478, Val AUC: 0.7460370384412383\n",
      "Epoch 82, Loss: 0.49018558859825134, Val Loss: 0.46518057584762573, Val AUC: 0.7445529662636101\n",
      "Epoch 83, Loss: 0.4917648434638977, Val Loss: 0.4632299542427063, Val AUC: 0.7412826898895912\n",
      "Epoch 84, Loss: 0.488223135471344, Val Loss: 0.46336013078689575, Val AUC: 0.7431393287411306\n",
      "Epoch 85, Loss: 0.4864838719367981, Val Loss: 0.463613361120224, Val AUC: 0.7433910090228005\n",
      "Epoch 86, Loss: 0.48825088143348694, Val Loss: 0.46487656235694885, Val AUC: 0.7425837812293267\n",
      "Epoch 87, Loss: 0.4868268370628357, Val Loss: 0.4642595946788788, Val AUC: 0.7405957700845678\n",
      "Epoch 88, Loss: 0.4874587655067444, Val Loss: 0.4617061913013458, Val AUC: 0.7429980489507232\n",
      "Epoch 89, Loss: 0.4855719208717346, Val Loss: 0.4554608166217804, Val AUC: 0.7420324504677384\n",
      "Epoch 90, Loss: 0.4822253882884979, Val Loss: 0.45121535658836365, Val AUC: 0.747431775220495\n",
      "Epoch 91, Loss: 0.481866717338562, Val Loss: 0.44968515634536743, Val AUC: 0.748825019344808\n",
      "Epoch 92, Loss: 0.48553186655044556, Val Loss: 0.4499768316745758, Val AUC: 0.750582489933535\n",
      "Epoch 93, Loss: 0.4803483486175537, Val Loss: 0.4515383243560791, Val AUC: 0.7507854536894848\n",
      "Epoch 94, Loss: 0.48091477155685425, Val Loss: 0.4519042372703552, Val AUC: 0.7507245160514142\n",
      "Epoch 95, Loss: 0.4779451787471771, Val Loss: 0.45218175649642944, Val AUC: 0.7555477319705194\n",
      "Epoch 96, Loss: 0.47870558500289917, Val Loss: 0.4514015316963196, Val AUC: 0.755067656824299\n",
      "Epoch 97, Loss: 0.47737032175064087, Val Loss: 0.44974738359451294, Val AUC: 0.7564442205296178\n",
      "Epoch 98, Loss: 0.47574785351753235, Val Loss: 0.44905608892440796, Val AUC: 0.757328487976366\n",
      "Epoch 99, Loss: 0.4727565050125122, Val Loss: 0.448078989982605, Val AUC: 0.7583175398001514\n",
      "Epoch 100, Loss: 0.4768279194831848, Val Loss: 0.4456057548522949, Val AUC: 0.7586097643217257\n",
      "Epoch 101, Loss: 0.4789392650127411, Val Loss: 0.44113245606422424, Val AUC: 0.7583442956400146\n",
      "Epoch 102, Loss: 0.47494667768478394, Val Loss: 0.4394177198410034, Val AUC: 0.758938652180351\n",
      "Epoch 103, Loss: 0.4746696352958679, Val Loss: 0.44167381525039673, Val AUC: 0.7584812840474593\n",
      "Epoch 104, Loss: 0.47173482179641724, Val Loss: 0.4460439383983612, Val AUC: 0.7576075025016896\n",
      "Epoch 105, Loss: 0.47484901547431946, Val Loss: 0.44737008213996887, Val AUC: 0.7595761091321822\n",
      "Epoch 106, Loss: 0.4729732871055603, Val Loss: 0.4460054934024811, Val AUC: 0.7581186248707361\n",
      "Epoch 107, Loss: 0.4722994267940521, Val Loss: 0.44399431347846985, Val AUC: 0.7580377043146087\n",
      "Epoch 108, Loss: 0.47248658537864685, Val Loss: 0.44235140085220337, Val AUC: 0.7585732689083557\n",
      "Epoch 109, Loss: 0.4714984595775604, Val Loss: 0.44178298115730286, Val AUC: 0.7592706746143576\n",
      "Epoch 110, Loss: 0.4703991711139679, Val Loss: 0.44195079803466797, Val AUC: 0.76008838830881\n",
      "Epoch 111, Loss: 0.46805259585380554, Val Loss: 0.4422852694988251, Val AUC: 0.7598173035128739\n",
      "Epoch 112, Loss: 0.46538224816322327, Val Loss: 0.4446445405483246, Val AUC: 0.7594513418370881\n",
      "Epoch 113, Loss: 0.4689653515815735, Val Loss: 0.4456130266189575, Val AUC: 0.7619892843794257\n",
      "Epoch 114, Loss: 0.47108012437820435, Val Loss: 0.4443013370037079, Val AUC: 0.7635094414903145\n",
      "Epoch 115, Loss: 0.4681597948074341, Val Loss: 0.44090670347213745, Val AUC: 0.7618396643795212\n",
      "Epoch 116, Loss: 0.4682410955429077, Val Loss: 0.4385087490081787, Val AUC: 0.7615004585435986\n",
      "Epoch 117, Loss: 0.47270503640174866, Val Loss: 0.43802979588508606, Val AUC: 0.7621363482077393\n",
      "Epoch 118, Loss: 0.46765613555908203, Val Loss: 0.4393031597137451, Val AUC: 0.7629429975974227\n",
      "Epoch 119, Loss: 0.4692169427871704, Val Loss: 0.44175052642822266, Val AUC: 0.762798825788062\n",
      "Epoch 120, Loss: 0.4661039113998413, Val Loss: 0.4440763592720032, Val AUC: 0.7635150576045396\n",
      "Epoch 121, Loss: 0.4646764397621155, Val Loss: 0.44649338722229004, Val AUC: 0.7626457353654138\n",
      "Epoch 122, Loss: 0.4651927053928375, Val Loss: 0.44484269618988037, Val AUC: 0.7608085010281407\n",
      "Epoch 123, Loss: 0.4668627977371216, Val Loss: 0.44069841504096985, Val AUC: 0.7607669679043363\n",
      "Epoch 124, Loss: 0.4666273593902588, Val Loss: 0.43696078658103943, Val AUC: 0.7613884907646453\n",
      "Epoch 125, Loss: 0.4650307595729828, Val Loss: 0.4365271329879761, Val AUC: 0.7637768133070785\n",
      "Epoch 126, Loss: 0.4656332731246948, Val Loss: 0.43664345145225525, Val AUC: 0.7618846679260695\n",
      "Epoch 127, Loss: 0.46674585342407227, Val Loss: 0.4370921850204468, Val AUC: 0.762315448142779\n",
      "Epoch 128, Loss: 0.4660869538784027, Val Loss: 0.43983733654022217, Val AUC: 0.7619733502879033\n",
      "Epoch 129, Loss: 0.4616222679615021, Val Loss: 0.44051069021224976, Val AUC: 0.7633227476732495\n",
      "Epoch 130, Loss: 0.4652223587036133, Val Loss: 0.4391809105873108, Val AUC: 0.7622838225161626\n",
      "Epoch 131, Loss: 0.4606320261955261, Val Loss: 0.4373774230480194, Val AUC: 0.7606322931120543\n",
      "Epoch 132, Loss: 0.4663527011871338, Val Loss: 0.434409499168396, Val AUC: 0.7596172317758773\n",
      "Epoch 133, Loss: 0.4645981788635254, Val Loss: 0.4336877167224884, Val AUC: 0.7577134436863084\n",
      "Epoch 134, Loss: 0.4636717736721039, Val Loss: 0.4379241168498993, Val AUC: 0.757882468200479\n",
      "Epoch 135, Loss: 0.4650146961212158, Val Loss: 0.44157859683036804, Val AUC: 0.7564348914362207\n",
      "Epoch 136, Loss: 0.4675459861755371, Val Loss: 0.4398960471153259, Val AUC: 0.7593017218371835\n",
      "Epoch 137, Loss: 0.4633731544017792, Val Loss: 0.43831923604011536, Val AUC: 0.7604038982475035\n",
      "Epoch 138, Loss: 0.4635995030403137, Val Loss: 0.43641573190689087, Val AUC: 0.7609582889517263\n",
      "Epoch 139, Loss: 0.46585243940353394, Val Loss: 0.43612971901893616, Val AUC: 0.7625593666187426\n",
      "Epoch 140, Loss: 0.4654709994792938, Val Loss: 0.43690383434295654, Val AUC: 0.7649742770773577\n",
      "Epoch 141, Loss: 0.46372121572494507, Val Loss: 0.43739134073257446, Val AUC: 0.7654005979874234\n",
      "Epoch 142, Loss: 0.46048200130462646, Val Loss: 0.44025829434394836, Val AUC: 0.7647718917251985\n",
      "Epoch 143, Loss: 0.46504008769989014, Val Loss: 0.44299790263175964, Val AUC: 0.7657481440328431\n",
      "Epoch 144, Loss: 0.46546176075935364, Val Loss: 0.4418143033981323, Val AUC: 0.7643876823427278\n",
      "Epoch 145, Loss: 0.4651964008808136, Val Loss: 0.43873757123947144, Val AUC: 0.7647264776985412\n",
      "Epoch 146, Loss: 0.4597204625606537, Val Loss: 0.4346919655799866, Val AUC: 0.7639171042135857\n",
      "Epoch 147, Loss: 0.4647023677825928, Val Loss: 0.43036985397338867, Val AUC: 0.7634182029568897\n",
      "Epoch 148, Loss: 0.46203693747520447, Val Loss: 0.4317179024219513, Val AUC: 0.7638555881717246\n",
      "Epoch 149, Loss: 0.45849624276161194, Val Loss: 0.43643489480018616, Val AUC: 0.7653856527798012\n",
      "Epoch 150, Loss: 0.46318385004997253, Val Loss: 0.4384647607803345, Val AUC: 0.7643217069942229\n",
      "Epoch 151, Loss: 0.46145719289779663, Val Loss: 0.43756353855133057, Val AUC: 0.7642058583124164\n",
      "Epoch 152, Loss: 0.46182331442832947, Val Loss: 0.43556204438209534, Val AUC: 0.7637956394175541\n",
      "Epoch 153, Loss: 0.4549300968647003, Val Loss: 0.43607187271118164, Val AUC: 0.7640512006020773\n",
      "Epoch 154, Loss: 0.46491092443466187, Val Loss: 0.4339326024055481, Val AUC: 0.7641299754667235\n",
      "Epoch 155, Loss: 0.46272772550582886, Val Loss: 0.4319664537906647, Val AUC: 0.7646733018661769\n",
      "Epoch 156, Loss: 0.4656125009059906, Val Loss: 0.43372851610183716, Val AUC: 0.7641804272038154\n",
      "Epoch 157, Loss: 0.46363943815231323, Val Loss: 0.43325671553611755, Val AUC: 0.7642004101218722\n",
      "Epoch 158, Loss: 0.4585762023925781, Val Loss: 0.4327571988105774, Val AUC: 0.7644676140149551\n",
      "Epoch 159, Loss: 0.4628700017929077, Val Loss: 0.4331097900867462, Val AUC: 0.7634979667054358\n",
      "Epoch 160, Loss: 0.462057888507843, Val Loss: 0.43264350295066833, Val AUC: 0.7646500164490575\n",
      "Epoch 161, Loss: 0.46237120032310486, Val Loss: 0.43295541405677795, Val AUC: 0.7661119600171477\n",
      "Epoch 162, Loss: 0.4586096704006195, Val Loss: 0.4341620206832886, Val AUC: 0.7644747974168712\n",
      "Epoch 163, Loss: 0.4626590609550476, Val Loss: 0.4363453686237335, Val AUC: 0.7647647083232828\n",
      "Epoch 164, Loss: 0.4630078971385956, Val Loss: 0.43636468052864075, Val AUC: 0.7657731646613345\n",
      "Epoch 165, Loss: 0.4572054445743561, Val Loss: 0.4347779154777527, Val AUC: 0.7651727815266637\n",
      "Epoch 166, Loss: 0.4582522213459015, Val Loss: 0.4315340220928192, Val AUC: 0.7614212731988431\n",
      "Epoch 167, Loss: 0.4589816927909851, Val Loss: 0.4284857511520386, Val AUC: 0.7611507668066976\n",
      "Epoch 168, Loss: 0.4606601297855377, Val Loss: 0.42648133635520935, Val AUC: 0.7612283848637624\n",
      "Epoch 169, Loss: 0.4624371826648712, Val Loss: 0.42720523476600647, Val AUC: 0.7622667316170587\n",
      "Epoch 170, Loss: 0.46092820167541504, Val Loss: 0.43415290117263794, Val AUC: 0.7660442494572707\n",
      "Epoch 171, Loss: 0.4606715440750122, Val Loss: 0.44186797738075256, Val AUC: 0.769585032223435\n",
      "Epoch 172, Loss: 0.4588419198989868, Val Loss: 0.4435107409954071, Val AUC: 0.7693366544408277\n",
      "Epoch 173, Loss: 0.4672798812389374, Val Loss: 0.4358595907688141, Val AUC: 0.7711307883666055\n",
      "Epoch 174, Loss: 0.45964735746383667, Val Loss: 0.4289810061454773, Val AUC: 0.770246110439748\n",
      "Epoch 175, Loss: 0.4620208442211151, Val Loss: 0.42753106355667114, Val AUC: 0.7653037433397736\n",
      "Epoch 176, Loss: 0.460683673620224, Val Loss: 0.4296850562095642, Val AUC: 0.7631742844809263\n",
      "Epoch 177, Loss: 0.4612977206707001, Val Loss: 0.4333716034889221, Val AUC: 0.7639087640040886\n",
      "Epoch 178, Loss: 0.457897812128067, Val Loss: 0.4337558448314667, Val AUC: 0.7660869393886562\n",
      "Epoch 179, Loss: 0.45646002888679504, Val Loss: 0.4331529438495636, Val AUC: 0.7666995436356777\n",
      "Epoch 180, Loss: 0.4571262001991272, Val Loss: 0.4322676956653595, Val AUC: 0.7681059977562411\n",
      "Epoch 181, Loss: 0.4601730704307556, Val Loss: 0.43070289492607117, Val AUC: 0.7677689376117998\n",
      "Epoch 182, Loss: 0.45817697048187256, Val Loss: 0.43141651153564453, Val AUC: 0.7669473430144942\n",
      "Epoch 183, Loss: 0.4573800563812256, Val Loss: 0.43541082739830017, Val AUC: 0.7660508544553959\n",
      "Epoch 184, Loss: 0.4584318995475769, Val Loss: 0.4384356737136841, Val AUC: 0.7670238042639779\n",
      "Epoch 185, Loss: 0.46289700269699097, Val Loss: 0.43540456891059875, Val AUC: 0.7642380623428234\n",
      "Epoch 186, Loss: 0.4621163606643677, Val Loss: 0.4292754530906677, Val AUC: 0.7635152255282207\n",
      "Epoch 187, Loss: 0.458610475063324, Val Loss: 0.42622503638267517, Val AUC: 0.7644666251310552\n",
      "Epoch 188, Loss: 0.4608374238014221, Val Loss: 0.4283273220062256, Val AUC: 0.7642492945712738\n",
      "Epoch 189, Loss: 0.45705896615982056, Val Loss: 0.4325600266456604, Val AUC: 0.7639654102591965\n",
      "Epoch 190, Loss: 0.4600112736225128, Val Loss: 0.4368329346179962, Val AUC: 0.765090704162955\n",
      "Epoch 191, Loss: 0.45798689126968384, Val Loss: 0.43655288219451904, Val AUC: 0.7662344136970795\n",
      "Epoch 192, Loss: 0.46230441331863403, Val Loss: 0.433148592710495, Val AUC: 0.7655957999376668\n",
      "Epoch 193, Loss: 0.45410314202308655, Val Loss: 0.4322754740715027, Val AUC: 0.7660770318914685\n",
      "Epoch 194, Loss: 0.46013331413269043, Val Loss: 0.43224433064460754, Val AUC: 0.7660188183486699\n",
      "Epoch 195, Loss: 0.4593859910964966, Val Loss: 0.43138065934181213, Val AUC: 0.766386925715937\n",
      "Epoch 196, Loss: 0.46055349707603455, Val Loss: 0.4308459162712097, Val AUC: 0.7663358955750542\n",
      "Epoch 197, Loss: 0.4606211185455322, Val Loss: 0.43027937412261963, Val AUC: 0.7655841572291071\n",
      "Epoch 198, Loss: 0.453642874956131, Val Loss: 0.4311980903148651, Val AUC: 0.7683101183197721\n",
      "Epoch 199, Loss: 0.46002787351608276, Val Loss: 0.4306209683418274, Val AUC: 0.7685280272833441\n",
      "Epoch 200, Loss: 0.45689496397972107, Val Loss: 0.4318426847457886, Val AUC: 0.7675894271966507\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.8408818125707417\n",
      "F1 Score: 0.03228454172366621\n",
      "Precision: 0.0165303149165079\n",
      "Recall: 0.6876456876456877\n",
      "ROC AUC: 0.7645606313634437\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "data_train.y = torch.tensor(undersampled_train['is_fraud'].values, dtype=torch.long)\n",
    "data_val.y = torch.tensor(validation['is_fraud'].values, dtype=torch.long)\n",
    "data_test.y = torch.tensor(test['is_fraud'].values, dtype=torch.long)\n",
    "\n",
    "# Creating masks for all nodes in train and validation sets\n",
    "data_train.train_mask = torch.tensor([True] * len(undersampled_train), dtype=torch.bool)\n",
    "data_val.val_mask = torch.tensor([True] * data_val.num_nodes, dtype=torch.bool) \n",
    "data_test.test_mask = torch.tensor([True] * data_test.num_nodes, dtype=torch.bool) \n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Define the GCN model architecture\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(num_node_features=data_train.num_node_features, num_classes=2).to(device)\n",
    "\n",
    "labels = undersampled_train['is_fraud'].values\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def compute_metrics(output, labels, zero_division=1):\n",
    "    _, predictions = torch.max(output, dim=1)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, zero_division=zero_division),\n",
    "        \"precision\": precision_score(labels, predictions, zero_division=zero_division),\n",
    "        \"recall\": recall_score(labels, predictions, zero_division=zero_division),\n",
    "        \"roc_auc\": roc_auc_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "# Adjust the training and evaluation functions to utilize the masks\n",
    "def train_and_evaluate(data_train, data_val, model, optimizer, epochs=200):\n",
    "    best_val_auc = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data_train)\n",
    "        # Use the train mask to calculate the loss\n",
    "        loss = F.nll_loss(out[data_train.train_mask], data_train.y[data_train.train_mask], weight=class_weights)\n",
    "        # loss = criterion(out[data_train.train_mask], data_train.y[data_train.train_mask]) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use the val mask to evaluate performance\n",
    "            val_out = model(data_val)\n",
    "            val_loss = F.nll_loss(val_out[data_val.val_mask], data_val.y[data_val.val_mask])\n",
    "            val_metrics = compute_metrics(val_out[data_val.val_mask], data_val.y[data_val.val_mask])\n",
    "            \n",
    "            # Print validation metrics\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val AUC: {val_metrics[\"roc_auc\"]}')\n",
    "            \n",
    "            # Save the best model state based on validation AUC\n",
    "            if val_metrics['roc_auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['roc_auc']\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "    # Load the best model state after training\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# Train and validate the model\n",
    "best_model = train_and_evaluate(data_train.to(device), data_val.to(device), model, optimizer)\n",
    "\n",
    "# Evaluate on the test set\n",
    "def evaluate_test_set(best_model, data_test):\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Use the test mask to evaluate performance\n",
    "        test_out = best_model(data_test)\n",
    "        test_metrics = compute_metrics(test_out[data_test.test_mask], data_test.y[data_test.test_mask])\n",
    "        \n",
    "        # Print test metrics\n",
    "        print(\"Test Set Evaluation:\")\n",
    "        print(f'Accuracy: {test_metrics[\"accuracy\"]}')\n",
    "        print(f'F1 Score: {test_metrics[\"f1\"]}')\n",
    "        print(f'Precision: {test_metrics[\"precision\"]}')\n",
    "        print(f'Recall: {test_metrics[\"recall\"]}')\n",
    "        print(f'ROC AUC: {test_metrics[\"roc_auc\"]}')\n",
    "\n",
    "# Call the function to evaluate the model on the test set\n",
    "evaluate_test_set(best_model, data_test.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7339551448822021, Val Loss: 0.8597514629364014, Val AUC: 0.5051275869202827\n",
      "Epoch 2, Loss: 0.6940368413925171, Val Loss: 0.9365063309669495, Val AUC: 0.5001319506970101\n",
      "Epoch 3, Loss: 0.6934372186660767, Val Loss: 0.9105685353279114, Val AUC: 0.5002561395883138\n",
      "Epoch 4, Loss: 0.6831748485565186, Val Loss: 0.8512023687362671, Val AUC: 0.5029814476424411\n",
      "Epoch 5, Loss: 0.6720594167709351, Val Loss: 0.7895699739456177, Val AUC: 0.5326597193032525\n",
      "Epoch 6, Loss: 0.6660507321357727, Val Loss: 0.7371712327003479, Val AUC: 0.5874377115092055\n",
      "Epoch 7, Loss: 0.655001699924469, Val Loss: 0.6981844305992126, Val AUC: 0.6321887568154625\n",
      "Epoch 8, Loss: 0.6523035168647766, Val Loss: 0.6700224876403809, Val AUC: 0.6431571211880579\n",
      "Epoch 9, Loss: 0.6442570686340332, Val Loss: 0.651323139667511, Val AUC: 0.6584497815648755\n",
      "Epoch 10, Loss: 0.6388890147209167, Val Loss: 0.6412959098815918, Val AUC: 0.6626838465777601\n",
      "Epoch 11, Loss: 0.6288988590240479, Val Loss: 0.638067364692688, Val AUC: 0.6610959789069945\n",
      "Epoch 12, Loss: 0.6219272613525391, Val Loss: 0.6398023962974548, Val AUC: 0.6626382646274214\n",
      "Epoch 13, Loss: 0.6114475131034851, Val Loss: 0.6422486305236816, Val AUC: 0.665929680727078\n",
      "Epoch 14, Loss: 0.6039998531341553, Val Loss: 0.6391001343727112, Val AUC: 0.6675713026339986\n",
      "Epoch 15, Loss: 0.6042109131813049, Val Loss: 0.6287280321121216, Val AUC: 0.6689773462744527\n",
      "Epoch 16, Loss: 0.5979911088943481, Val Loss: 0.6107181906700134, Val AUC: 0.6730091938588593\n",
      "Epoch 17, Loss: 0.5947579741477966, Val Loss: 0.5908917784690857, Val AUC: 0.6733820963801326\n",
      "Epoch 18, Loss: 0.5964131355285645, Val Loss: 0.5767654776573181, Val AUC: 0.6781445425848486\n",
      "Epoch 19, Loss: 0.5917299389839172, Val Loss: 0.5696000456809998, Val AUC: 0.6767359427728036\n",
      "Epoch 20, Loss: 0.5880532264709473, Val Loss: 0.5657020807266235, Val AUC: 0.6806267531232313\n",
      "Epoch 21, Loss: 0.584100604057312, Val Loss: 0.5659024119377136, Val AUC: 0.6841255922854815\n",
      "Epoch 22, Loss: 0.5813689827919006, Val Loss: 0.5684766173362732, Val AUC: 0.6843435012490536\n",
      "Epoch 23, Loss: 0.5751414895057678, Val Loss: 0.5712560415267944, Val AUC: 0.6852716154347684\n",
      "Epoch 24, Loss: 0.5759493708610535, Val Loss: 0.5729852318763733, Val AUC: 0.6859624161426452\n",
      "Epoch 25, Loss: 0.5729617476463318, Val Loss: 0.5745177268981934, Val AUC: 0.6843275671575313\n",
      "Epoch 26, Loss: 0.5703675746917725, Val Loss: 0.5750706195831299, Val AUC: 0.684522190703984\n",
      "Epoch 27, Loss: 0.5685607194900513, Val Loss: 0.5756746530532837, Val AUC: 0.6869060539397732\n",
      "Epoch 28, Loss: 0.5650180578231812, Val Loss: 0.5761734843254089, Val AUC: 0.6898792360054643\n",
      "Epoch 29, Loss: 0.5677189230918884, Val Loss: 0.5722863078117371, Val AUC: 0.6894684387068115\n",
      "Epoch 30, Loss: 0.5619696974754333, Val Loss: 0.5640143752098083, Val AUC: 0.6911482912384738\n",
      "Epoch 31, Loss: 0.5584733486175537, Val Loss: 0.5526586174964905, Val AUC: 0.6901648555289134\n",
      "Epoch 32, Loss: 0.5551557540893555, Val Loss: 0.5416936278343201, Val AUC: 0.6901244792126902\n",
      "Epoch 33, Loss: 0.5534741282463074, Val Loss: 0.5324276089668274, Val AUC: 0.6901960706754204\n",
      "Epoch 34, Loss: 0.553222119808197, Val Loss: 0.5252898931503296, Val AUC: 0.6940137222754271\n",
      "Epoch 35, Loss: 0.5495478510856628, Val Loss: 0.5211414098739624, Val AUC: 0.6969193617049224\n",
      "Epoch 36, Loss: 0.5515145063400269, Val Loss: 0.5175352096557617, Val AUC: 0.6986430609755515\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "data_train.y = torch.tensor(undersampled_train['is_fraud'].values, dtype=torch.long)\n",
    "data_val.y = torch.tensor(validation['is_fraud'].values, dtype=torch.long)\n",
    "data_test.y = torch.tensor(test['is_fraud'].values, dtype=torch.long)\n",
    "\n",
    "# Creating masks for all nodes in train and validation sets\n",
    "data_train.train_mask = torch.tensor([True] * len(undersampled_train), dtype=torch.bool)\n",
    "data_val.val_mask = torch.tensor([True] * data_val.num_nodes, dtype=torch.bool) \n",
    "data_test.test_mask = torch.tensor([True] * data_test.num_nodes, dtype=torch.bool)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Define the GCN model architecture\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 32)  \n",
    "        self.conv2 = GCNConv(32, 64)  \n",
    "        self.conv3 = GCNConv(64, num_classes) \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))  # Additional layer with ReLU\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)  # Output layer\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(num_node_features=data_train.num_node_features, num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def compute_metrics(output, labels, zero_division=1):\n",
    "    _, predictions = torch.max(output, dim=1)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, zero_division=zero_division),\n",
    "        \"precision\": precision_score(labels, predictions, zero_division=zero_division),\n",
    "        \"recall\": recall_score(labels, predictions, zero_division=zero_division),\n",
    "        \"roc_auc\": roc_auc_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "labels = undersampled_train['is_fraud'].values\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "# Adjust the training and evaluation functions to utilize the masks\n",
    "def train_and_evaluate(data_train, data_val, model, optimizer, epochs=200):\n",
    "    best_val_auc = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data_train)\n",
    "        # Use the train mask to calculate the loss\n",
    "        loss = F.nll_loss(out[data_train.train_mask], data_train.y[data_train.train_mask], weight=class_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use the val mask to evaluate performance\n",
    "            val_out = model(data_val)\n",
    "            val_loss = F.nll_loss(val_out[data_val.val_mask], data_val.y[data_val.val_mask])\n",
    "            val_metrics = compute_metrics(val_out[data_val.val_mask], data_val.y[data_val.val_mask])\n",
    "            \n",
    "            # Print validation metrics\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}, Val AUC: {val_metrics[\"roc_auc\"]}')\n",
    "            \n",
    "            # Save the best model state based on validation AUC\n",
    "            if val_metrics['roc_auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['roc_auc']\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "    # Load the best model state after training\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# Train and validate the model\n",
    "best_model = train_and_evaluate(data_train.to(device), data_val.to(device), model, optimizer)\n",
    "\n",
    "# Evaluate on the test set\n",
    "def evaluate_test_set(best_model, data_test):\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Use the test mask to evaluate performance\n",
    "        test_out = best_model(data_test)\n",
    "        test_metrics = compute_metrics(test_out[data_test.test_mask], data_test.y[data_test.test_mask])\n",
    "        \n",
    "        # Print test metrics\n",
    "        print(\"Test Set Evaluation:\")\n",
    "        print(f'Accuracy: {test_metrics[\"accuracy\"]}')\n",
    "        print(f'F1 Score: {test_metrics[\"f1\"]}')\n",
    "        print(f'Precision: {test_metrics[\"precision\"]}')\n",
    "        print(f'Recall: {test_metrics[\"recall\"]}')\n",
    "        print(f'ROC AUC: {test_metrics[\"roc_auc\"]}')\n",
    "\n",
    "# Call the function to evaluate the model on the test set\n",
    "evaluate_test_set(best_model, data_test.to(device))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
